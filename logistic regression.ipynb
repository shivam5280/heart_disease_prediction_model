{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb3e4787-295c-46fd-bc01-6f6cd65b912c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 150 candidates, totalling 750 fits\n",
      "\n",
      "âœ… Best Parameters: {'tol': 0.0001, 'solver': 'saga', 'penalty': 'elasticnet', 'max_iter': 500, 'l1_ratio': 0.2222222222222222, 'class_weight': 'balanced', 'C': 51.79474679231202}\n",
      "ðŸ“Š Accuracy: 0.5581\n",
      "ðŸŽ¯ F1 Score: 0.5864\n",
      "\n",
      "ðŸ”Ž Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.68      0.77        25\n",
      "           1       0.30      0.38      0.33         8\n",
      "           2       0.50      0.50      0.50         4\n",
      "           3       0.22      0.50      0.31         4\n",
      "           4       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.56        43\n",
      "   macro avg       0.38      0.41      0.38        43\n",
      "weighted avg       0.64      0.56      0.59        43\n",
      "\n",
      "\n",
      "ðŸ“Š Confusion Matrix:\n",
      " [[17  3  2  2  1]\n",
      " [ 2  3  0  3  0]\n",
      " [ 0  1  2  1  0]\n",
      " [ 0  2  0  2  0]\n",
      " [ 0  1  0  1  0]]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Fetch dataset\n",
    "heart_disease = fetch_ucirepo(id=45)\n",
    "\n",
    "# Extract data\n",
    "X = heart_disease.data.features\n",
    "y = heart_disease.data.targets\n",
    "\n",
    "# Handle missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "\n",
    "# Remove highly correlated features\n",
    "corr_matrix = X_imputed.corr().abs()\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.90)]\n",
    "X_imputed.drop(columns=to_drop, inplace=True)\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X_imputed), columns=X_imputed.columns)\n",
    "\n",
    "# Outlier Removal (Using IQR Method)\n",
    "Q1 = X_scaled.quantile(0.25)\n",
    "Q3 = X_scaled.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "X_filtered = X_scaled[~((X_scaled < (Q1 - 1.5 * IQR)) | (X_scaled > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "y_filtered = y.loc[X_filtered.index]  # Adjust target variable accordingly\n",
    "\n",
    "# Adding Polynomial Features\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "X_poly = poly.fit_transform(X_filtered)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_poly, y_filtered, test_size=0.2, random_state=42, stratify=y_filtered)\n",
    "\n",
    "# Handle Class Imbalance using SMOTE with fewer neighbors\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Use fewer neighbors to avoid the mismatch error\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42, k_neighbors=2)  # Reduced neighbors\n",
    "X_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "# Feature Selection using Recursive Feature Elimination (RFE)\n",
    "base_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "rfe = RFE(base_model, n_features_to_select=15)\n",
    "X_train_rfe = rfe.fit_transform(X_train_sm, y_train_sm)\n",
    "X_test_rfe = rfe.transform(X_test)\n",
    "\n",
    "# Hyperparameter tuning with RandomizedSearchCV\n",
    "param_grid = {\n",
    "    'C': np.logspace(-4, 4, 50),                   # Regularization strength\n",
    "    'penalty': ['l1', 'l2', 'elasticnet'],         # Regularization type\n",
    "    'solver': ['liblinear', 'lbfgs', 'saga'],      # Optimizers\n",
    "    'class_weight': ['balanced', None],            # Class balancing\n",
    "    'max_iter': [500, 1000, 2000],                 # Max iterations\n",
    "    'tol': [1e-4, 1e-3, 1e-2],                     # Tolerance for stopping criteria\n",
    "    'l1_ratio': np.linspace(0, 1, 10)              # ElasticNet mixing ratio (only valid with elasticnet penalty)\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# RandomizedSearchCV for faster hyperparameter tuning\n",
    "random_search = RandomizedSearchCV(\n",
    "    LogisticRegression(random_state=42),\n",
    "    param_grid,\n",
    "    n_iter=150,             \n",
    "    scoring='accuracy',\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "random_search.fit(X_train_rfe, y_train_sm)\n",
    "\n",
    "# Best model\n",
    "best_log_reg = random_search.best_estimator_\n",
    "\n",
    "# **Ensemble Learning**: Combine Logistic Regression with Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "voting_clf = VotingClassifier(estimators=[('lr', best_log_reg), ('rf', rf)], voting='soft')\n",
    "voting_clf.fit(X_train_rfe, y_train_sm)\n",
    "\n",
    "# Model evaluation\n",
    "y_pred = voting_clf.predict(X_test_rfe)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Display results\n",
    "print(\"\\nâœ… Best Parameters:\", random_search.best_params_)\n",
    "print(f\"ðŸ“Š Accuracy: {accuracy:.4f}\")\n",
    "print(f\"ðŸŽ¯ F1 Score: {f1:.4f}\")\n",
    "print(\"\\nðŸ”Ž Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"\\nðŸ“Š Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194d25db-eef3-452a-bbce-dbf2fce7bf7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
